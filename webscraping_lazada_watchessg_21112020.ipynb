{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch driver (DO NOT ATTEMPT!)\n",
    "# Will get blocked\n",
    "#for i in range(0,3):\n",
    "#    i = i + 1\n",
    "#    url = 'https://www.lazada.sg/watchessg/?from=wangpu&lang=en&page={}&q=All-Products&rating=4'.format(i)\n",
    "#    driver = webdriver.Chrome()\n",
    "#    driver.get(url)\n",
    "#    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch driver\n",
    "def launch_driver(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup\n",
    "def create_soup(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve product names\n",
    "def extract_names(soup):\n",
    "    names = soup.find_all('div', {'class': 'c16H9d'})\n",
    "    names_list = []\n",
    "    for i in names:\n",
    "        names_list.append(i.text)\n",
    "    \n",
    "    return names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve product prices\n",
    "def extract_prices(soup):\n",
    "    prices = soup.find_all('span', {'class':'c13VH6'})\n",
    "    prices_list = []\n",
    "    for i in prices:\n",
    "        prices_list.append(i.text)\n",
    "    \n",
    "    return prices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve product ratings\n",
    "def extract_ratings(soup):\n",
    "    ratings = soup.find_all('div', {'class': 'c15YQ9'})\n",
    "    ratings_list = []\n",
    "    for i in ratings:\n",
    "        ratings_list.append(str(i).count('<i class=\"c3dn4k c3EEAg\">'))\n",
    "    \n",
    "    return ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store scraped data into a dataframe\n",
    "def create_dataframe(names_list, prices_list, ratings_list):\n",
    "    df = pd.DataFrame({'Name': names_list,\n",
    "                      'Price': prices_list,\n",
    "                      'Rating': ratings_list})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entire pipeline\n",
    "def full_pipeline(url):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        page_source = driver.page_source\n",
    "        soup = create_soup(page_source)\n",
    "        df = create_dataframe(extract_names(soup), extract_prices(soup), extract_ratings(soup))\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three pages for URL 'https://www.lazada.sg/watchessg/?from=wangpu&lang=en&q=All-Products&rating=4' .\n",
    "# Hence, use below URLs for scraping to cover all three pages.\n",
    "# Scrape these URLs separately and not in a loop to avoid your IP address from getting blocked.\n",
    "i = 1 # After scraping i = 1, i.e. first page, proceed to scrape i = 2 (second page), i = 3 (third page).\n",
    "url = 'https://www.lazada.sg/watchessg/?from=wangpu&lang=en&page={}&q=All-Products&rating=4'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commented line of code below to run the full scraping pipeline\n",
    "#if i == 1:\n",
    "    df_page1 = full_pipeline(url)\n",
    "elif i == 2:\n",
    "    df_page2 = full_pipeline(url)\n",
    "elif i == 3:\n",
    "    df_page3 = full_pipeline(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes above\n",
    "frames = [df_page1, df_page2, df_page3]\n",
    "df_overall = pd.concat(frames, axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export concatenated dataframe\n",
    "df_overall.to_csv('webscraping_lazada_watchessg_21112020.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
