{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(url = 'https://www.todayonline.com/', name = 'today'):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = r'C:\\Users\\65961\\OneDrive\\Desktop\\Data_Products\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    delay = 5\n",
    "    try:\n",
    "        myElem = WebDriverWait(driver, delay)\n",
    "        print(\"Page {} is ready!\".format(driver.current_url))\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time!\")\n",
    "    current_page_source = driver.page_source\n",
    "    soup = BeautifulSoup(current_page_source, 'html.parser')\n",
    "    \n",
    "    # Extract info\n",
    "    hotnews = soup.find_all('section', {'categoryid':'7'})\n",
    "    #hotnews_list = hotnews[0].text.split(sep = 'ago')\n",
    "    hotnews_soup = hotnews[0].find_all('a')\n",
    "    hotnews_list = []\n",
    "    \n",
    "    for item in hotnews_soup:\n",
    "        hotnews_dict = {'Title': item.text,\n",
    "                       'Type': 'hotnews',\n",
    "                        'URL': url,\n",
    "                        'HREF': item['href'],\n",
    "                        'Name': name,\n",
    "                       'Date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
    "                       'Datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        hotnews_list.append(hotnews_dict)\n",
    "    \n",
    "    popularnews = soup.find_all('section', {'categoryid':'3'})\n",
    "    #popularnews_list = popularnews[0].text.split(sep = 'ago')\n",
    "    popularnews_soup = popularnews[0].find_all('a')\n",
    "    popularnews_list = []\n",
    "    \n",
    "    for item in popularnews_soup:\n",
    "        popularnews_dict = {'Title': item.text,\n",
    "                       'Type': 'popularnews',\n",
    "                        'URL': url,\n",
    "                        'HREF': item['href'],\n",
    "                        'Name': name,\n",
    "                       'Date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
    "                       'Datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        popularnews_list.append(popularnews_dict)\n",
    "    \n",
    "    worldnews = soup.find_all('section', {'categoryid':'5'})\n",
    "    #worldnews_list = worldnews[0].text.split(sep = 'ago')\n",
    "    worldnews_soup = worldnews[0].find_all('a')\n",
    "    worldnews_list = []\n",
    "    \n",
    "    for item in worldnews_soup:\n",
    "        worldnews_dict = {'Title': item.text,\n",
    "                       'Type': 'worldnews',\n",
    "                        'URL': url,\n",
    "                        'HREF': item['href'],\n",
    "                        'Name': name,\n",
    "                       'Date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
    "                       'Datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        worldnews_list.append(worldnews_dict)\n",
    "    \n",
    "\n",
    "    \n",
    "    mega_list = [hotnews_list, popularnews_list, worldnews_list]\n",
    "    \n",
    "    #mega_list2 = []\n",
    "\n",
    "    #i = 0\n",
    "\n",
    "    #for item in mega_list:\n",
    "    #    if i == 0:\n",
    "    #        news_type = 'hotnews'\n",
    "    #    elif i == 1:\n",
    "    #        news_type = 'popularnews'\n",
    "    #    elif i == 2:\n",
    "    #        news_type = 'worldnews'\n",
    "    #    for item2 in item:\n",
    "    #        item2_dict = {'News': item2,\n",
    "    #                     'News_Original': item2 + ' ago',\n",
    "    #                      'News_Type': news_type,\n",
    "    #                      'URL': url,\n",
    "    #                      'Name': name,\n",
    "    #                     'Date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
    "    #                     'Datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    #        mega_list2.append(item2_dict)\n",
    "    #    i = i + 1\n",
    "\n",
    "    mega_df = pd.DataFrame(mega_list)\n",
    "    \n",
    "    mega_df = pd.concat([pd.DataFrame(hotnews_list), pd.DataFrame(popularnews_list), pd.DataFrame(worldnews_list)], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    mega_df = mega_df.drop_duplicates(subset = ['HREF'], keep = 'last').reset_index(drop = True)\n",
    "    \n",
    "    mega_df.to_csv('C:\\\\Users\\\\65961\\\\OneDrive\\\\Desktop\\\\Data_Products\\\\' + '{}_news_scraper_v3_'.format(name) + datetime.datetime.now().strftime('%Y%m%d%H%M%S') + '.csv', index = False)\n",
    "    \n",
    "    return mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(df):\n",
    "    \n",
    "    href_list = df['HREF']\n",
    "    url_list = df['URL']\n",
    "\n",
    "    link_list = []\n",
    "    for item in range(0, df.shape[0]):\n",
    "        link_list.append(url_list[item][:-1] + href_list[item])\n",
    "        \n",
    "    df2 = df\n",
    "    df2['Link'] = link_list\n",
    "        \n",
    "    article_list = []    \n",
    "    for item in link_list:\n",
    "        time.sleep(3)\n",
    "        url = item\n",
    "        driver = webdriver.Chrome(executable_path = r'C:\\Users\\65961\\OneDrive\\Desktop\\Data_Products\\chromedriver.exe')\n",
    "        driver.get(url)\n",
    "        delay = 5\n",
    "        try:\n",
    "            myElem = WebDriverWait(driver, delay)\n",
    "            print(\"Page {} is ready!\".format(driver.current_url))\n",
    "        except TimeoutException:\n",
    "            print(\"Loading took too much time!\")\n",
    "        \n",
    "        current_page_source = driver.page_source\n",
    "        soup = BeautifulSoup(current_page_source, 'html.parser')\n",
    "        \n",
    "        article = soup.find_all('div', {'id': 'article_detail_body'})\n",
    "        \n",
    "        if len(article) < 1:\n",
    "            continue\n",
    "        \n",
    "        article2 = article[0].find_all('p')\n",
    "        \n",
    "        article3 = ''\n",
    "        for item2 in article2:\n",
    "            article3 = article3 + ' ' + item2.text\n",
    "        article4 = article3.replace('\\n', '').replace('\\xa0', ' ').strip()\n",
    "        \n",
    "        article_dict = {'Link': item,\n",
    "                       'Article': article4}\n",
    "        article_list.append(article_dict)\n",
    "        \n",
    "    #article_df = pd.DataFrame(article_list)\n",
    "    \n",
    "    #df2 = pd.merge(df2, article_df, how = 'left', on = ['Link'])\n",
    "    \n",
    "    #df2 = df2.reset_index(drop = True)\n",
    "    \n",
    "    #df2_name = df2['Name']\n",
    "    \n",
    "    #name = df2_name[0]\n",
    "    \n",
    "    #df2.to_csv('C:\\\\Users\\\\65961\\\\OneDrive\\\\Desktop\\\\Data_Products\\\\' + '{}_news_scraper_article_v3_'.format(name) + datetime.datetime.now().strftime('%Y%m%d%H%M%S') + '.csv', index = False)\n",
    "    \n",
    "    return df2, article_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df, list_):\n",
    "    \n",
    "    list_df = pd.DataFrame(list_)\n",
    "    \n",
    "    df = pd.merge(df, list_df, how = 'left', on = ['Link'])\n",
    "    \n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    df_name = df['Name']\n",
    "    \n",
    "    name = df_name[0]\n",
    "    \n",
    "    df.to_csv('C:\\\\Users\\\\65961\\\\OneDrive\\\\Desktop\\\\Data_Products\\\\' + '{}_news_scraper_article_v3_'.format(name) + datetime.datetime.now().strftime('%Y%m%d%H%M%S') + '.csv', index = False)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://www.todayonline.com/ is ready!\n"
     ]
    }
   ],
   "source": [
    "mega_df = extract_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\65961\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://www.todayonline.com/singapore/new-covid-19-cases-dec-26 is ready!\n",
      "Page https://www.todayonline.com/singapore/covid-19-not-likely-be-eradicated-sars-ncid-chief-says is ready!\n",
      "Page https://www.todayonline.com/singapore/moh-orders-concord-international-hospital-suspend-healthcare-services-due-significant is ready!\n"
     ]
    }
   ],
   "source": [
    "mega_df2, article_list = extract_article(mega_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df3 = merge_data(mega_df2, article_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
